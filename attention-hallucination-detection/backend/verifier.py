"""
åŸºäº Qwen 2.5 æ¨¡å‹çš„äº‹å®ä¸€è‡´æ€§æ ¡éªŒå™¨
ä½¿ç”¨çœŸå®çš„ Transformer æ¨¡å‹æ£€æµ‹å¤§è¯­è¨€æ¨¡å‹çš„å¹»è§‰ç”Ÿæˆ
"""

import torch
import json
import numpy as np
import re
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass, field, asdict
from transformers import (
    AutoModelForCausalLM, 
    AutoTokenizer,
    LogitsProcessorList,
    LogitsProcessor,
    GenerationConfig
)
import warnings
import time
import os

warnings.filterwarnings("ignore")


@dataclass
class AttentionAnalysis:
    """æ³¨æ„åŠ›åˆ†æç»“æœ"""
    token: str
    token_id: int
    position: int
    system_attention: float    # å¯¹ç³»ç»Ÿæç¤ºçš„æ³¨æ„åŠ›æ€»å’Œ
    user_attention: float      # å¯¹ç”¨æˆ·æç¤ºçš„æ³¨æ„åŠ›æ€»å’Œ
    factuality_score: float    # äº‹å®æ€§å¾—åˆ† (system / (system + user))
    attention_weights: List[float]


@dataclass
class VerificationResult:
    """ç®€åŒ–çš„éªŒè¯ç»“æœ"""
    sequence: str
    tokens: List[str]
    factuality_score: float      # æ ¸å¿ƒæŒ‡æ ‡ï¼šäº‹å®æ€§å¾—åˆ†
    avg_system_attention: float  # å¹³å‡ç³»ç»Ÿæç¤ºæ³¨æ„åŠ›
    avg_user_attention: float    # å¹³å‡ç”¨æˆ·æç¤ºæ³¨æ„åŠ›
    final_verdict: str           # æœ€ç»ˆè£å†³
    is_hallucination: bool
    analyses: List[Dict[str, Any]]
    verdict_details: Dict[str, Any]


class FactualConsistencyVerifier(LogitsProcessor):
    """
    åŸºäºæ³¨æ„åŠ›æœºåˆ¶çš„äº‹å®ä¸€è‡´æ€§æ ¡éªŒå™¨
    ç®€åŒ–ç‰ˆï¼šæ£€æµ‹æ•°å­—åºåˆ—å¼€å§‹æ—¶å¯¹ç³»ç»Ÿæç¤ºçš„æœ€å¤§æ³¨æ„åŠ›
    """
    
    def __init__(
        self,
        tokenizer,
        context_length: int,
        system_prompt_length: int,
        model=None,
        max_attention_threshold: float = 0.1,  # 10% æœ€å¤§æ³¨æ„åŠ›é˜ˆå€¼
        min_sequence_length: int = 6,
        attention_layer_index: int = -1,  # Use last layer by default
        verbose: bool = True
    ):
        self.tokenizer = tokenizer
        self.context_length = context_length
        self.system_prompt_length = system_prompt_length
        self.model = model
        self.max_attention_threshold = max_attention_threshold
        self.min_sequence_length = min_sequence_length
        self.attention_layer_index = attention_layer_index
        self.verbose = verbose
        
        # å†…éƒ¨çŠ¶æ€ç¼“å†²
        self.reset_buffers()
        
        # è®°å½•æ‰€æœ‰åˆ†ææ•°æ®
        self.verification_results = []
        
        # æ³¨æ„åŠ›ç¼“å­˜
        self.attention_cache = {}
        self.generation_step = 0
        
        # æ£€æµ‹çŠ¶æ€
        self.number_sequence_started = False
        self.max_system_attention = 0.0
    
    def reset_buffers(self):
        """é‡ç½®å†…éƒ¨ç¼“å†²åŒº"""
        self.generated_text = ""  # ç´¯ç§¯çš„ç”Ÿæˆæ–‡æœ¬
        self.generated_tokens = []  # æ‰€æœ‰ç”Ÿæˆçš„token
        self.generation_step = 0
        self.attention_cache = {}
        
        # åºåˆ—è¿½è¸ªçŠ¶æ€
        self.current_sequence = ""  # å½“å‰æ­£åœ¨æ„å»ºçš„åºåˆ—
        self.sequence_start_pos = -1  # åºåˆ—å¼€å§‹ä½ç½®
        self.sequence_tokens = []  # åºåˆ—ä¸­çš„tokens
        
        # æ£€æµ‹çŠ¶æ€
        self.number_sequence_started = False
        self.max_system_attention = 0.0
    
    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:
        """LogitsProcessor çš„ä¸»è¦è°ƒç”¨æ¥å£"""
        self.generation_step += 1
        
        # è·å–æœ€åç”Ÿæˆçš„ token
        if input_ids.shape[1] > self.context_length:
            last_token_id = input_ids[0, -1].item()
            last_token = self.tokenizer.decode([last_token_id])
            current_position = input_ids.shape[1] - 1
            
            # ç´¯ç§¯ç”Ÿæˆçš„æ–‡æœ¬
            self.generated_text += last_token
            self.generated_tokens.append((last_token_id, last_token, current_position))
            
            if self.verbose and self.generation_step <= 5:  # åªæ‰“å°å‰å‡ ä¸ªtoken
                print(f"   ğŸ”¤ Token {self.generation_step}: '{last_token}'")
            
            # æ£€æµ‹å’Œåˆ†æåºåˆ—
            self._track_sequence(last_token, current_position)
        
        return scores
    
    def _track_sequence(self, token: str, position: int):
        """è¿½è¸ªæ•°å­—åºåˆ—å¹¶æ£€æµ‹æ³¨æ„åŠ›"""
        # å¿½ç•¥å‰å‡ ä¸ªtokenï¼ˆæ€»æ˜¯æœ‰é«˜å³°ï¼‰
        if position < self.context_length + 5:
            return
            
        # æ£€æŸ¥tokenæ˜¯å¦åŒ…å«æ•°å­—ï¼ˆæ•°å­—åºåˆ—çš„å¼€å§‹ï¼‰
        contains_digit = any(char.isdigit() for char in token)
        
        if contains_digit and not self.number_sequence_started:
            # æ•°å­—åºåˆ—å¼€å§‹ï¼
            self.number_sequence_started = True
            if self.verbose:
                print(f"\nğŸ”¢ Number sequence started with token: '{token}' at position {position}")
            
        # ç»§ç»­è¿½è¸ªåºåˆ—æ„å»º
        is_sequence_char = bool(re.match(r'[A-Z0-9\-\s]+$', token.upper()))
        
        if is_sequence_char:
            if self.sequence_start_pos == -1:
                self.sequence_start_pos = len(self.generated_text) - len(token)
                self.current_sequence = token.upper()
                self.sequence_tokens = [(position, token)]
            else:
                self.current_sequence += token.upper()
                self.sequence_tokens.append((position, token))
        else:
            if self.current_sequence and self.sequence_start_pos != -1:
                effective_length = len([c for c in self.current_sequence if c.isalnum()])
                
                if effective_length >= self.min_sequence_length:
                    if self.verbose:
                        print(f"   ğŸ” Complete sequence detected: {self.current_sequence}")
                    self._record_sequence_result()
                
                self.current_sequence = ""
                self.sequence_start_pos = -1
                self.sequence_tokens = []
    
    def _record_sequence_result(self):
        """è®°å½•åºåˆ—æ£€æµ‹ç»“æœ"""
        sequence = self.current_sequence.strip()
        
        # æ„å»ºåºåˆ—çš„tokenä¿¡æ¯
        tokens = [token for _, token in self.sequence_tokens]
        
        # å¦‚æœæ£€æµ‹åˆ°æ•°å­—åºåˆ—ï¼Œæ£€æŸ¥æ‰€æœ‰ç”Ÿæˆçš„tokençš„æ³¨æ„åŠ›
        if self.number_sequence_started:
            # æ£€æŸ¥æ‰€æœ‰ç”Ÿæˆçš„tokenä½ç½®ï¼Œä¸ä»…ä»…æ˜¯åºåˆ—token
            # å› ä¸ºæœ€å¤§æ³¨æ„åŠ›å¯èƒ½åœ¨å…¶ä»–ä½ç½®
            for position in self.attention_cache.keys():
                if position >= self.context_length:  # åªæ£€æŸ¥ç”Ÿæˆçš„éƒ¨åˆ†
                    self._update_max_system_attention(position)
            
            # æ‰“å°æ³¨æ„åŠ›çŸ©é˜µä»¥ä¾¿è°ƒè¯•
            if self.verbose:
                self._print_attention_matrix()
        
        # åŸºäºæœ€å¤§æ³¨æ„åŠ›åˆ¤æ–­æ˜¯å¦ä¸ºå¹»è§‰
        if not self.number_sequence_started:
            # å¦‚æœæ²¡æœ‰æ£€æµ‹åˆ°æ•°å­—åºåˆ—å¼€å§‹ï¼Œæ— æ³•åˆ¤æ–­
            verdict = "NO_NUMBER_DETECTED"
            is_hallucination = False
            factuality_score = 0.5
        else:
            # æ ¹æ®æœ€å¤§æ³¨æ„åŠ›åˆ¤æ–­
            is_hallucination = self.max_system_attention <= self.max_attention_threshold
            if is_hallucination:
                verdict = "HALLUCINATION_DETECTED"
                factuality_score = 0.1
            else:
                verdict = "VERIFIED"
                factuality_score = 0.9
            
            if self.verbose:
                print(f"   ğŸ“Š Final max attention: {self.max_system_attention:.3f}")
                print(f"   ğŸ“Š Threshold: {self.max_attention_threshold:.3f}")
                print(f"   ğŸ“Š Final verdict: {'HALLUCINATION' if is_hallucination else 'NON-HALLUCINATION'}")
        
        result = VerificationResult(
            sequence=sequence,
            tokens=tokens,
            factuality_score=factuality_score,
            avg_system_attention=self.max_system_attention,
            avg_user_attention=0.0,  # ç®€åŒ–ç‰ˆä¸å†è®¡ç®—ç”¨æˆ·æ³¨æ„åŠ›
            final_verdict=verdict,
            is_hallucination=is_hallucination,
            analyses=[],  # ç®€åŒ–ç‰ˆä¸å†éœ€è¦è¯¦ç»†åˆ†æ
            verdict_details={
                "max_system_attention": self.max_system_attention,
                "threshold": self.max_attention_threshold,
                "number_sequence_started": self.number_sequence_started
            }
        )
        
        self.verification_results.append(result)
        
        if self.verbose:
            print(f"\nğŸ“Š Sequence Result: {sequence}")
            print(f"   Verdict: {verdict}")
            print(f"   Max System Attention: {self.max_system_attention:.3f}")
            print(f"   Is Hallucination: {is_hallucination}")
    
    def finalize_sequences(self):
        """åœ¨ç”Ÿæˆç»“æŸæ—¶åˆ†æä»»ä½•æœªå®Œæˆçš„åºåˆ—"""
        if self.current_sequence and self.sequence_start_pos != -1:
            # è®¡ç®—æœ‰æ•ˆé•¿åº¦ï¼ˆå»é™¤ç©ºæ ¼å’Œè¿å­—ç¬¦ï¼‰
            effective_length = len([c for c in self.current_sequence if c.isalnum()])
            
            if effective_length >= self.min_sequence_length:
                # åºåˆ—æ»¡è¶³æœ€å°é•¿åº¦è¦æ±‚ï¼Œè¿›è¡Œåˆ†æ
                if self.verbose:
                    print(f"   ğŸ” Final sequence detected: {self.current_sequence}")
                
                # è®°å½•åºåˆ—ç»“æœ
                self._record_sequence_result()
            
            # é‡ç½®åºåˆ—è¿½è¸ªçŠ¶æ€
            self.current_sequence = ""
            self.sequence_start_pos = -1
            self.sequence_tokens = []
    
    def _print_attention_heatmap(self, full_matrix):
        """æ‰“å°æ³¨æ„åŠ›çƒ­å›¾"""
        print("\n   ğŸ“Š Attention Heatmap:")
        
        # åˆ›å»ºé¢œè‰²æ˜ å°„ - ä½¿ç”¨ä¸åŒçš„å­—ç¬¦è¡¨ç¤ºä¸åŒçš„æ³¨æ„åŠ›å¼ºåº¦
        def get_char_for_value(val):
            if val < 0.01: return 'Â·'    # 0-1%
            elif val < 0.02: return 'â–'  # 1-2%
            elif val < 0.05: return 'â–‚'  # 2-5%
            elif val < 0.10: return 'â–ƒ'  # 5-10%
            elif val < 0.15: return 'â–„'  # 10-15%
            elif val < 0.20: return 'â–…'  # 15-20%
            elif val < 0.30: return 'â–†'  # 20-30%
            elif val < 0.50: return 'â–‡'  # 30-50%
            else: return 'â–ˆ'              # 50%+
        
        # è®¡ç®—å„åŒºåŸŸçš„å®½åº¦ï¼ˆé˜²æ­¢è´Ÿæ•°ï¼‰
        skip_width = 5
        system_width = max(0, self.system_prompt_length - 5)
        user_width = max(0, self.context_length - self.system_prompt_length)
        gen_width = 10  # å‡å°‘ç”ŸæˆåŒºåŸŸçš„æ˜¾ç¤ºå®½åº¦
        
        # é™åˆ¶æ€»å®½åº¦
        max_total_width = 80
        if skip_width + system_width + user_width + gen_width > max_total_width:
            # æŒ‰æ¯”ä¾‹ç¼©å‡
            total = skip_width + system_width + user_width + gen_width
            system_width = int(system_width * max_total_width / total)
            user_width = int(user_width * max_total_width / total)
            gen_width = max_total_width - skip_width - system_width - user_width
        
        # æ‰“å°è¾¹ç•Œæ ‡è®°
        print("\n      " + "â”€" * skip_width + "â”¬" + "â”€" * system_width + "â”¬" + 
              "â”€" * user_width + "â”¬" + "â”€" * gen_width)
        
        # æ‰“å°åŒºåŸŸæ ‡ç­¾
        skip_label = "SKIP"
        system_label = "SYSTEM" if system_width >= 6 else "SYS"
        user_label = "USER" if user_width >= 4 else "U"
        gen_label = "GEN"
        
        # è®¡ç®—æ ‡ç­¾ä½ç½®
        skip_padding = max(0, (skip_width - len(skip_label)) // 2)
        system_padding = max(0, (system_width - len(system_label)) // 2)
        user_padding = max(0, (user_width - len(user_label)) // 2)
        gen_padding = max(0, (gen_width - len(gen_label)) // 2)
        
        # æ„å»ºæ ‡ç­¾è¡Œ
        label_line = "      "
        label_line += " " * skip_padding + skip_label[:skip_width]
        label_line += " " * max(0, skip_width - skip_padding - len(skip_label)) + "â”‚"
        
        if system_width > 0:
            label_line += " " * system_padding + system_label[:system_width]
            label_line += " " * max(0, system_width - system_padding - len(system_label))
        label_line += "â”‚"
        
        if user_width > 0:
            label_line += " " * user_padding + user_label[:user_width]
            label_line += " " * max(0, user_width - user_padding - len(user_label))
        label_line += "â”‚"
        
        label_line += " " * gen_padding + gen_label
        print(label_line)
        
        print("      " + "â”€" * skip_width + "â”´" + "â”€" * system_width + "â”´" + 
              "â”€" * user_width + "â”´" + "â”€" * gen_width)
        
        # æ‰“å°æ¯ä¸€è¡Œ
        for i, row in enumerate(full_matrix[:min(20, len(full_matrix))]):  # æœ€å¤šæ˜¾ç¤º20è¡Œ
            # æ‰“å°è¡Œå·
            print(f"   {i:2d} ", end="")
            
            # æ‰“å°æ³¨æ„åŠ›å€¼ - é™åˆ¶å®½åº¦ä»¥é¿å…æ¢è¡Œ
            max_display_width = min(80, self.context_length + 10)  # æœ€å¤šæ˜¾ç¤º80ä¸ªå­—ç¬¦
            for j, val in enumerate(row[:min(max_display_width, len(row))]):
                # åœ¨è¾¹ç•Œå¤„æ·»åŠ åˆ†éš”ç¬¦
                if j == 5:
                    print("â”‚", end="")
                elif j == self.system_prompt_length:
                    print("â”‚", end="")
                elif j == self.context_length:
                    print("â”‚", end="")
                    
                # æ‰“å°æ³¨æ„åŠ›å­—ç¬¦
                print(get_char_for_value(val), end="")
            
            # åœ¨è¡Œæœ«æ˜¾ç¤ºè¯¥è¡Œçš„æœ€å¤§å€¼ä½ç½®å’Œå€¼
            if row:
                max_val = max(row)
                max_idx = row.index(max_val)
                print(f"  â† max: {max_val*100:.1f}% at pos {max_idx}", end="")
            
            print()  # æ¢è¡Œ
        
        if len(full_matrix) > 20:
            print(f"      ... ({len(full_matrix) - 20} more rows)")
        
        # æ‰“å°å›¾ä¾‹
        print("\n      Legend: Â· <1%  â– 1-2%  â–‚ 2-5%  â–ƒ 5-10%  â–„ 10-15%  â–… 15-20%  â–† 20-30%  â–‡ 30-50%  â–ˆ >50%")
        print("      Regions: â”‚<-SKIP->â”‚<-----SYSTEM----->â”‚<--USER-->â”‚<-GEN->")
        
        # æ‰“å°ç³»ç»Ÿæç¤ºåŒºåŸŸçš„æœ€å¤§æ³¨æ„åŠ›
        if full_matrix:
            max_system_attention = 0.0
            max_system_pos = -1
            for row in full_matrix:
                for j in range(5, min(self.system_prompt_length, len(row))):
                    if row[j] > max_system_attention:
                        max_system_attention = row[j]
                        max_system_pos = j
            
            print(f"\n      ğŸ“ Max attention in SYSTEM region: {max_system_attention*100:.2f}% at position {max_system_pos}")
            print(f"      ğŸ¯ Threshold: 10.00% - Verdict: {'âœ“ NON-HALLUCINATION' if max_system_attention > 0.1 else 'âœ— HALLUCINATION'}")
    
    def _print_attention_matrix(self):
        """æ‰“å°æ³¨æ„åŠ›çŸ©é˜µç”¨äºè°ƒè¯•"""
        print("\n   ğŸ¯ Attention Matrix Visualization:")
        print(f"      Context boundary: {self.context_length}")
        print(f"      System prompt boundary: {self.system_prompt_length}")
        
        # è·å–å®Œæ•´æ³¨æ„åŠ›çŸ©é˜µ
        full_matrix = self.get_full_attention_matrix()
        if not full_matrix:
            print("      No attention matrix available")
            return
            
        print(f"      Matrix shape: {len(full_matrix)} generated tokens")
        
        # é¦–å…ˆæ‰“å°å®Œæ•´çš„æ³¨æ„åŠ›çƒ­å›¾
        self._print_attention_heatmap(full_matrix)
        
        # æ‰“å°å‰å‡ è¡Œçš„æ³¨æ„åŠ›åˆ†å¸ƒ
        for i in range(min(10, len(full_matrix))):
            row = full_matrix[i]
            print(f"\n      Generated token {i} attention distribution:")
            
            # æ‰¾å‡ºè¿™ä¸€è¡Œä¸­çš„æœ€é«˜æ³¨æ„åŠ›å€¼
            if row:
                max_val = max(row)
                max_idx = row.index(max_val)
                
                # æ‰“å°å‰5ä¸ªæœ€é«˜çš„æ³¨æ„åŠ›å€¼
                sorted_indices = sorted(range(len(row)), key=lambda k: row[k], reverse=True)[:5]
                print(f"        Top 5 positions:")
                for idx in sorted_indices:
                    val = row[idx]
                    if idx < self.system_prompt_length:
                        region = "SYSTEM" if idx >= 5 else "SKIP"
                    elif idx < self.context_length:
                        region = "USER"
                    else:
                        region = "GEN"
                    print(f"          Pos {idx:3d} ({region:6s}): {val*100:6.2f}%")
                
                # è®¡ç®—å„åŒºåŸŸçš„æ³¨æ„åŠ›æ€»å’Œ
                skip_attn = sum(row[j] for j in range(min(5, len(row))))
                system_attn = sum(row[j] for j in range(5, min(self.system_prompt_length, len(row))))
                user_attn = sum(row[j] for j in range(self.system_prompt_length, min(self.context_length, len(row))))
                gen_attn = sum(row[j] for j in range(self.context_length, len(row)))
                
                print(f"        Region totals: SKIP={skip_attn*100:.1f}%, SYSTEM={system_attn*100:.1f}%, USER={user_attn*100:.1f}%, GEN={gen_attn*100:.1f}%")
    
    def _update_max_system_attention(self, position: int):
        """æ›´æ–°ç³»ç»Ÿæç¤ºéƒ¨åˆ†çš„æœ€å¤§æ³¨æ„åŠ›"""
        if position not in self.attention_cache:
            return
            
        attention = self.attention_cache[position]
        if isinstance(attention, torch.Tensor):
            attention_np = attention.cpu().numpy()
        else:
            attention_np = np.array(attention)
            
        # Check attention to ALL tokens in the prompt (not just system prompt)
        # This will help us debug where the high attention is
        prompt_attention = attention_np[:self.context_length]
        
        # Find max attention in entire prompt
        max_prompt_attention = float(np.max(prompt_attention))
        max_prompt_idx = np.argmax(prompt_attention)
        
        # Check system prompt but skip first 5 tokens (they always have high attention)
        start_idx = 5  # Skip first 5 tokens
        end_idx = self.system_prompt_length
        
        # Debug: Always show attention distribution for number sequences
        if self.number_sequence_started and self.verbose:
            print(f"\n   ğŸ“Š Attention Debug at position {position}:")
            print(f"      Attention check range: {start_idx} to {end_idx} (system prompt except first 5)")
            print(f"      Context length: {self.context_length}")
            print(f"      Total attention length: {len(attention_np)}")
            
            # Show top 10 attention positions in the entire prompt
            top_10_indices = np.argsort(prompt_attention)[-10:][::-1]
            print(f"      Top 10 attention positions in prompt:")
            for idx in top_10_indices:
                print(f"        Position {idx}: {prompt_attention[idx]:.4f} ({prompt_attention[idx]*100:.2f}%)")
        
        if start_idx < end_idx and end_idx <= len(attention_np):
            prompt_attention_slice = attention_np[start_idx:end_idx]
            current_max = float(np.max(prompt_attention_slice)) if len(prompt_attention_slice) > 0 else 0.0
            
            # Debug: print the actual slice being used
            if self.verbose and self.number_sequence_started:
                print(f"      Slicing attention[{start_idx}:{end_idx}], length={len(prompt_attention_slice)}")
                if len(prompt_attention_slice) > 0:
                    print(f"      Raw max value in slice: {current_max}")
                    # Show the actual values in the prompt attention slice
                    top_5_in_slice = np.argsort(prompt_attention_slice)[-5:][::-1]
                    print(f"      Top 5 positions in prompt slice:")
                    for i in top_5_in_slice:
                        actual_pos = start_idx + i
                        print(f"        Relative pos {i} (absolute {actual_pos}): {prompt_attention_slice[i]:.4f}")
            
            # Debug: print if there's a discrepancy
            if max_prompt_attention > current_max * 1.5:  # Significant difference
                if self.verbose:
                    print(f"   ğŸ” High attention found outside system prompt range!")
                    print(f"      Max in entire prompt: {max_prompt_attention:.3f} at position {max_prompt_idx}")
                    print(f"      Max in system prompt (5-{end_idx}): {current_max:.3f}")
                    # Let's see what tokens have high attention
                    top_indices = np.argsort(prompt_attention)[-5:][::-1]
                    top_values = [f"{prompt_attention[i]:.4f}" for i in top_indices]
                    print(f"      Top 5 attention positions: {top_indices} with values: {top_values}")
            
            # Update global max with system prompt attention
            if current_max > self.max_system_attention:
                self.max_system_attention = current_max
                if self.verbose:
                    print(f"   ğŸ“Š New max attention found: {self.max_system_attention:.3f} at position {position}")
    

    

    
    def update_attention_cache(self, position: int, attention_weights):
        """æ›´æ–°æ³¨æ„åŠ›ç¼“å­˜"""
        self.attention_cache[position] = attention_weights
    
    def get_full_attention_matrix(self):
        """è·å–å®Œæ•´çš„æ³¨æ„åŠ›çŸ©é˜µ - åŒ…å«æ‰€æœ‰ç”Ÿæˆtokençš„æ³¨æ„åŠ›æƒé‡"""
        if not self.attention_cache:
            return []
        
        # æ‰¾å‡ºæ‰€æœ‰ä½ç½®çš„æœ€å¤§é•¿åº¦
        max_position = max(self.attention_cache.keys())
        min_position = min(self.attention_cache.keys())
        
        # ç¡®å®šç”Ÿæˆéƒ¨åˆ†çš„èµ·å§‹ä½ç½®ï¼ˆcontext_lengthä¹‹åçš„ç¬¬ä¸€ä¸ªä½ç½®ï¼‰
        start_position = max(self.context_length, min_position)
        
        # æ„å»ºå®Œæ•´çŸ©é˜µ - åŒ…å«æ‰€æœ‰ç”Ÿæˆçš„token
        full_matrix = []
        missing_positions = []
        
        # ä¸ºäº†å½¢æˆæ­£ç¡®çš„ä¸‰è§’å½¢æ¨¡å¼ï¼Œéœ€è¦ç¡®ä¿æ¯è¡Œçš„é•¿åº¦é€’å¢
        for pos in range(start_position, max_position + 1):
            if pos in self.attention_cache:
                weights = self.attention_cache[pos]
                if isinstance(weights, torch.Tensor):
                    weights = weights.cpu().numpy().tolist()
                elif isinstance(weights, np.ndarray):
                    weights = weights.tolist()
                
                # ç¡®ä¿æƒé‡é•¿åº¦æ­£ç¡®ï¼ˆåº”è¯¥æ˜¯ pos + 1ï¼‰
                if len(weights) < pos + 1:
                    # å¦‚æœæƒé‡å¤ªçŸ­ï¼Œåœ¨æœ«å°¾è¡¥é›¶
                    weights = weights + [0.0] * (pos + 1 - len(weights))
                elif len(weights) > pos + 1:
                    # å¦‚æœæƒé‡å¤ªé•¿ï¼Œæˆªæ–­
                    weights = weights[:pos + 1]
                
                full_matrix.append(weights)
            else:
                # è®°å½•ç¼ºå¤±çš„ä½ç½®ï¼Œç”¨é›¶å¡«å……
                missing_positions.append(pos)
                # è·å–é¢„æœŸçš„æƒé‡é•¿åº¦
                expected_length = pos + 1  # æ³¨æ„åŠ›åº”è¯¥è¦†ç›–åˆ°å½“å‰ä½ç½®
                full_matrix.append([0.0] * expected_length)
        
        if missing_positions and self.verbose:
            print(f"   âš ï¸  Warning: Missing attention data for positions: {missing_positions}")
        
        if self.verbose:
            print(f"   âœ… Full attention matrix: {len(full_matrix)} tokens from position {start_position} to {max_position}")
            print(f"   âœ… Total positions in cache: {len(self.attention_cache)}")
            print(f"   âœ… System prompt boundary: {self.system_prompt_length}")
            print(f"   âœ… User prompt boundary: {self.context_length}")
            # æ‰“å°çŸ©é˜µå½¢çŠ¶ä»¥éªŒè¯ä¸‰è§’å½¢æ¨¡å¼
            if full_matrix:
                print(f"   âœ… Matrix shape - First row length: {len(full_matrix[0])}, Last row length: {len(full_matrix[-1])}")
                # éªŒè¯æ˜¯å¦ä¸ºä¸‰è§’å½¢æ¨¡å¼
                is_triangular = all(len(full_matrix[i]) == start_position + i + 1 for i in range(len(full_matrix)))
                print(f"   âœ… Triangular pattern: {'YES' if is_triangular else 'NO'}")
        
        return full_matrix


def capture_attention_hook(verifier, layer_idx, verbose=False):
    """åˆ›å»ºä¸€ä¸ªé’©å­å‡½æ•°æ¥æ•è·æ³¨æ„åŠ›æƒé‡"""
    def hook(module, input, output):
        try:
            # å¤„ç†ä¸åŒçš„è¾“å‡ºæ ¼å¼
            attention_weights = None
            
            # å°è¯•å¤šç§æ–¹å¼è·å–æ³¨æ„åŠ›æƒé‡
            if hasattr(output, 'attentions') and output.attentions is not None:
                attention_weights = output.attentions
            elif isinstance(output, tuple) and len(output) > 1:
                # æœ‰äº›æ¨¡å‹å°†æ³¨æ„åŠ›ä½œä¸ºå…ƒç»„çš„ç¬¬äºŒä¸ªå…ƒç´ è¿”å›
                for item in output:
                    if isinstance(item, torch.Tensor) and len(item.shape) == 4:
                        # æ£€æŸ¥æ˜¯å¦ä¸ºæ³¨æ„åŠ›æƒé‡çš„å½¢çŠ¶ [batch, heads, seq, seq]
                        attention_weights = item
                        break
            elif isinstance(output, dict) and 'attentions' in output:
                attention_weights = output['attentions']
            
            if attention_weights is not None:
                if isinstance(attention_weights, (list, tuple)):
                    # æ ¹æ®é…ç½®é€‰æ‹©ç‰¹å®šå±‚
                    layer_index = verifier.attention_layer_index
                    if layer_index >= 0 and layer_index < len(attention_weights):
                        attention_weights = attention_weights[layer_index]
                    elif layer_index < 0 and abs(layer_index) <= len(attention_weights):
                        # æ”¯æŒè´Ÿç´¢å¼•ï¼Œ-1 ä¸ºæœ€åä¸€å±‚
                        attention_weights = attention_weights[layer_index]
                    else:
                        # å¦‚æœç´¢å¼•è¶Šç•Œï¼Œä½¿ç”¨æœ€åä¸€å±‚
                        if verbose:
                            print(f"   [DEBUG] Layer index {layer_index} out of range, using last layer")
                        attention_weights = attention_weights[-1]
                
                if isinstance(attention_weights, torch.Tensor) and attention_weights.dim() >= 3:
                    # è·å–æœ€åä¸€ä¸ªtokençš„æ³¨æ„åŠ›åˆ†å¸ƒ
                    # å½¢çŠ¶é€šå¸¸æ˜¯ [batch, num_heads, seq_len, seq_len]
                    if attention_weights.dim() == 4:
                        # å¹³å‡æ‰€æœ‰æ³¨æ„åŠ›å¤´
                        avg_attention = attention_weights[0, :, -1, :].mean(dim=0)
                    else:
                        # å¯èƒ½å·²ç»æ˜¯å¹³å‡åçš„æ³¨æ„åŠ›
                        avg_attention = attention_weights[0, -1, :]
                    
                    current_pos = avg_attention.shape[0] - 1
                    verifier.update_attention_cache(current_pos, avg_attention)
                    
                    if verbose:
                        print(f"   [DEBUG] Captured attention at position {current_pos}, shape: {avg_attention.shape}")
                        print(f"   [DEBUG] Attention sum: {avg_attention.sum().item():.4f}")
                        # æ˜¾ç¤ºæ³¨æ„åŠ›åˆ†å¸ƒçš„å‰å‡ ä¸ªå’Œåå‡ ä¸ªå€¼
                        if current_pos > verifier.context_length:
                            context_attn = avg_attention[:verifier.context_length].sum().item()
                            generated_attn = avg_attention[verifier.context_length:].sum().item()
                            print(f"   [DEBUG] Context attention: {context_attn:.4f}, Generated attention: {generated_attn:.4f}")
        except Exception as e:
            if verbose:
                print(f"   [DEBUG] Error in attention hook: {e}")
    
    return hook


def load_test_cases(file_path: str = "test_cases.json") -> List[Dict[str, Any]]:
    """ä»JSONæ–‡ä»¶åŠ è½½æµ‹è¯•ç”¨ä¾‹"""
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"æµ‹è¯•ç”¨ä¾‹æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
    
    with open(file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    return data['test_cases']


def run_verification(
    model_name: str = "Qwen/Qwen2.5-0.5B-Instruct",
    test_cases_file: str = "test_cases.json",
    attention_layer_index: int = -1
):
    """è¿è¡ŒéªŒè¯æµ‹è¯•"""
    print("ğŸš€ åˆå§‹åŒ– Qwen 2.5 æ¨¡å‹å’Œåˆ†è¯å™¨...")
    print(f"   æ¨¡å‹: {model_name}")
    
    # åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
    
    # è®¾ç½® pad_token
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # æ£€æµ‹å¯ç”¨è®¾å¤‡
    device = "cuda" if torch.cuda.is_available() else "mps" if torch.backends.mps.is_available() else "cpu"
    print(f"   ä½¿ç”¨è®¾å¤‡: {device}")
    
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch.float32 if device == "cpu" else torch.float16,
        trust_remote_code=True,
        attn_implementation="eager"  # ä½¿ç”¨ eager attention ä»¥æ”¯æŒ output_attentions
    )
    
    # ç§»åŠ¨æ¨¡å‹åˆ°è®¾å¤‡
    model = model.to(device)
    
    # è·å–æ¨¡å‹å±‚æ•°
    num_layers = None
    # å°è¯•ä»æ¨¡å‹é…ç½®ä¸­è·å–å±‚æ•°
    if hasattr(model, 'config'):
        if hasattr(model.config, 'num_hidden_layers'):
            num_layers = model.config.num_hidden_layers
        elif hasattr(model.config, 'n_layer'):
            num_layers = model.config.n_layer
        elif hasattr(model.config, 'num_layers'):
            num_layers = model.config.num_layers
    
    # å¦‚æœä»é…ç½®ä¸­æ— æ³•è·å–ï¼Œå°è¯•è®¡ç®—transformerå±‚æ•°
    if num_layers is None:
        transformer_layers = 0
        for name, _ in model.named_modules():
            if any(pattern in name.lower() for pattern in ['layer.', 'layers.', 'h.', 'transformer.h.']):
                # è®¡ç®—åŒ…å«æ•°å­—çš„å±‚æ¨¡å—
                parts = name.split('.')
                for part in parts:
                    if part.isdigit():
                        layer_idx = int(part)
                        transformer_layers = max(transformer_layers, layer_idx + 1)
        if transformer_layers > 0:
            num_layers = transformer_layers
    
    print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
    if num_layers:
        print(f"   æ¨¡å‹å±‚æ•°: {num_layers} å±‚ (æœ‰æ•ˆç´¢å¼•: 0 åˆ° {num_layers-1}, æˆ– -1 åˆ° -{num_layers})")
        
        # ç¡®å®šå®é™…ä½¿ç”¨çš„å±‚
        if attention_layer_index >= 0:
            if attention_layer_index < num_layers:
                actual_layer = attention_layer_index
                print(f"   æ³¨æ„åŠ›å±‚: ä½¿ç”¨ç¬¬ {actual_layer} å±‚ (ç´¢å¼•: {attention_layer_index})")
            else:
                actual_layer = num_layers - 1
                print(f"   âš ï¸  è­¦å‘Š: æŒ‡å®šçš„å±‚ç´¢å¼• {attention_layer_index} è¶…å‡ºèŒƒå›´ï¼Œå°†ä½¿ç”¨æœ€åä¸€å±‚ (ç¬¬ {actual_layer} å±‚)")
        else:
            if abs(attention_layer_index) <= num_layers:
                actual_layer = num_layers + attention_layer_index
                print(f"   æ³¨æ„åŠ›å±‚: ä½¿ç”¨ç¬¬ {actual_layer} å±‚ (ç´¢å¼•: {attention_layer_index})")
            else:
                actual_layer = 0
                print(f"   âš ï¸  è­¦å‘Š: æŒ‡å®šçš„å±‚ç´¢å¼• {attention_layer_index} è¶…å‡ºèŒƒå›´ï¼Œå°†ä½¿ç”¨ç¬¬ä¸€å±‚ (ç¬¬ 0 å±‚)")
    else:
        print(f"   æ³¨æ„åŠ›å±‚ç´¢å¼•: {attention_layer_index}")
    
    # åŠ è½½æµ‹è¯•ç”¨ä¾‹
    print(f"\nğŸ“‹ åŠ è½½æµ‹è¯•ç”¨ä¾‹: {test_cases_file}")
    test_cases = load_test_cases(test_cases_file)
    print(f"   æ‰¾åˆ° {len(test_cases)} ä¸ªæµ‹è¯•ç”¨ä¾‹")
    
    all_results = []
    
    for i, test_case in enumerate(test_cases):
        print(f"\n{'='*80}")
        print(f"ğŸ“ æµ‹è¯•ç”¨ä¾‹ {i+1}/{len(test_cases)}: {test_case['name']}")
        print(f"   ç±»åˆ«: {test_case['category']}")
        print(f"   æè¿°: {test_case['description']}")
        
        # æ„å»ºè¾“å…¥
        messages = [
            {"role": "system", "content": test_case['system_prompt']},
            {"role": "user", "content": test_case['user_prompt']}
        ]
        
        # è®¡ç®—ç³»ç»Ÿæç¤ºçš„é•¿åº¦ - éœ€è¦æ›´å‡†ç¡®çš„æ–¹æ³•
        # ç¼–ç å®Œæ•´æ¶ˆæ¯ï¼Œç„¶åæ‰¾åˆ°ç³»ç»Ÿæç¤ºçš„ç»“æŸä½ç½®
        # å…ˆç¼–ç åªæœ‰ç³»ç»Ÿæ¶ˆæ¯çš„ç‰ˆæœ¬
        system_only_text = tokenizer.apply_chat_template(
            [{"role": "system", "content": test_case['system_prompt']}],
            tokenize=False,
            add_generation_prompt=True  # æ·»åŠ ç”Ÿæˆæç¤ºä»¥åŒ¹é…å®Œæ•´ç‰ˆæœ¬
        )
        
        # ç¼–ç ç³»ç»Ÿ+ç”¨æˆ·æ¶ˆæ¯ï¼Œæ‰¾åˆ°ç”¨æˆ·æ¶ˆæ¯å¼€å§‹çš„ä½ç½®
        full_messages_text = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        
        # Tokenize both to find the boundary
        system_only_ids = tokenizer(system_only_text, return_tensors="pt")['input_ids'][0]
        full_ids = tokenizer(full_messages_text, return_tensors="pt")['input_ids'][0]
        
        # è°ƒè¯•è¾“å‡º
        print(f"\n   ğŸ” Token è¾¹ç•Œè°ƒè¯•:")
        print(f"      System-only length: {len(system_only_ids)}")
        print(f"      Full message length: {len(full_ids)}")
        
        # è§£ç å‰20ä¸ªtokenæŸ¥çœ‹
        print(f"      First 20 tokens of full message:")
        for i in range(min(20, len(full_ids))):
            token = tokenizer.decode([full_ids[i].item()])
            print(f"        {i}: '{token}'")
        
        # æ›´ä¿å®ˆçš„æ–¹æ³•ï¼šä½¿ç”¨ç”¨æˆ·æ¶ˆæ¯çš„æ ‡è®°æ¥æ‰¾è¾¹ç•Œ
        user_message_marker = "user"  # æˆ–å…¶ä»–æ ‡è®°
        
        # æ‰¾åˆ°åŒ…å« "user" è§’è‰²æ ‡è®°çš„ä½ç½®
        system_prompt_length = len(system_only_ids) - 2  # å‡å»ç”Ÿæˆæç¤ºéƒ¨åˆ†
        
        # æŸ¥æ‰¾ç”¨æˆ·æ¶ˆæ¯å¼€å§‹çš„ä½ç½®
        # æ–¹æ³•1: æŸ¥æ‰¾ "<|im_start|>user" çš„æ¨¡å¼
        user_start_found = False
        for i in range(10, len(full_ids) - 10):
            # è§£ç ä¸€æ®µè¾ƒé•¿çš„åºåˆ—æ¥æ‰¾åˆ°å®Œæ•´çš„æ¨¡å¼
            decoded_segment = tokenizer.decode(full_ids[i:i+10].tolist())
            if "<|im_start|>user" in decoded_segment:
                # æ‰¾åˆ°äº†useræ ‡è®°çš„å¼€å§‹ï¼Œä½†éœ€è¦æ‰¾åˆ°å†…å®¹å¼€å§‹çš„ä½ç½®
                # é€šå¸¸åœ¨ "<|im_start|>user\n" ä¹‹å
                for j in range(i, min(i+10, len(full_ids))):
                    token = tokenizer.decode([full_ids[j].item()])
                    if token == '\n' and j > i:  # æ‰¾åˆ°useråçš„æ¢è¡Œç¬¦
                        system_prompt_length = j + 1  # å†…å®¹åœ¨æ¢è¡Œç¬¦ä¹‹åå¼€å§‹
                        user_start_found = True
                        print(f"      Found user content start at position {system_prompt_length}")
                        break
                if user_start_found:
                    break
        
        # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ³•
        if not user_start_found:
            # è§£ç ä¸€äº›å…³é”®ä½ç½®çš„tokensæ¥è°ƒè¯•
            print(f"      Checking tokens around estimated boundary:")
            check_start = max(0, len(system_only_ids) - 10)
            check_end = min(len(full_ids), len(system_only_ids) + 20)
            for i in range(check_start, check_end):
                if i < len(full_ids):
                    token = tokenizer.decode([full_ids[i].item()])
                    print(f"        {i}: '{token}'")
            
            # ä½¿ç”¨ä¿å®ˆä¼°è®¡
            system_prompt_length = len(system_only_ids)
        
        # ç¼–ç å®Œæ•´çš„è¾“å…¥
        text = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        
        inputs = tokenizer(text, return_tensors="pt")
        # å°†è¾“å…¥ç§»åˆ°æ¨¡å‹æ‰€åœ¨çš„è®¾å¤‡
        inputs = {k: v.to(device) for k, v in inputs.items()}
        context_length = inputs['input_ids'].shape[1]
        
        print(f"\n   ğŸ“ Tokenè¾¹ç•Œä¿¡æ¯:")
        print(f"      ç³»ç»Ÿæç¤ºé•¿åº¦: {system_prompt_length} tokens")
        print(f"      æ€»ä¸Šä¸‹æ–‡é•¿åº¦: {context_length} tokens")
        print(f"      ç”¨æˆ·æç¤ºé•¿åº¦: {context_length - system_prompt_length} tokens")
        
        # æ˜¾ç¤ºç³»ç»Ÿæç¤ºéƒ¨åˆ†çš„æœ€åå‡ ä¸ªtokenï¼Œå¸®åŠ©éªŒè¯è¾¹ç•Œ
        if system_prompt_length > 5:
            print(f"      System prompt last 5 tokens (positions {system_prompt_length-5} to {system_prompt_length-1}):")
            for i in range(max(0, system_prompt_length-5), system_prompt_length):
                if i < len(full_ids):
                    token = tokenizer.decode([full_ids[i].item()])
                    print(f"        {i}: '{token}'")
        
        # åˆ›å»ºæ ¡éªŒå™¨
        verifier = FactualConsistencyVerifier(
            tokenizer=tokenizer,
            context_length=context_length,
            system_prompt_length=system_prompt_length,
            model=model,
            max_attention_threshold=0.1,  # 10% threshold (adjusted based on empirical data)
            attention_layer_index=attention_layer_index,
            verbose=True
        )
        
        # è®¾ç½®ç”Ÿæˆé…ç½®
        generation_config = GenerationConfig(
            max_new_tokens=150,
            temperature=0.7,
            do_sample=True,
            top_p=0.9,
            repetition_penalty=1.1
        )
        
        # ç”Ÿæˆå¹¶éªŒè¯
        print("\nğŸ” å¼€å§‹ç”Ÿæˆå¹¶éªŒè¯...")
        start_time = time.time()
        
        with torch.no_grad():
            # æ³¨å†Œé’©å­æ¥æ•è·æ³¨æ„åŠ›
            hooks = []
            hook_count = 0
            for name, module in model.named_modules():
                # å°è¯•å¤šç§æ¨¡å—åç§°æ¨¡å¼
                if any(pattern in name.lower() for pattern in ['attn', 'attention', 'self_attn']):
                    if hasattr(module, 'forward'):
                        hook = module.register_forward_hook(capture_attention_hook(verifier, hook_count, verbose=False))
                        hooks.append(hook)
                        hook_count += 1
            
            try:
                outputs = model.generate(
                    **inputs,
                    generation_config=generation_config,
                    logits_processor=LogitsProcessorList([verifier]),
                    output_attentions=True,
                    output_scores=True,
                    return_dict_in_generate=True
                )
            finally:
                # ç§»é™¤é’©å­
                for hook in hooks:
                    hook.remove()
                
                # åˆ†æä»»ä½•æœªå®Œæˆçš„åºåˆ—
                verifier.finalize_sequences()
        
        generation_time = time.time() - start_time
        
        # å°è¯•ä»è¾“å‡ºä¸­æå–æ³¨æ„åŠ›æƒé‡
        if hasattr(outputs, 'attentions') and outputs.attentions is not None:
            if verifier.verbose:
                print(f"   [DEBUG] Found attentions in generate output")
            # å¤„ç†ç”Ÿæˆè¿‡ç¨‹ä¸­çš„æ³¨æ„åŠ›æƒé‡
            try:
                for step_idx, step_attentions in enumerate(outputs.attentions):
                    if step_attentions is not None and len(step_attentions) > 0:
                        # æ ¹æ®é…ç½®é€‰æ‹©ç‰¹å®šå±‚çš„æ³¨æ„åŠ›
                        layer_index = verifier.attention_layer_index
                        total_layers = len(step_attentions)
                        
                        if layer_index >= 0 and layer_index < total_layers:
                            selected_layer_attention = step_attentions[layer_index]
                            actual_layer = layer_index
                        elif layer_index < 0 and abs(layer_index) <= total_layers:
                            selected_layer_attention = step_attentions[layer_index]
                            actual_layer = total_layers + layer_index
                        else:
                            # å¦‚æœç´¢å¼•è¶Šç•Œï¼Œä½¿ç”¨æœ€åä¸€å±‚
                            if verifier.verbose:
                                print(f"   [DEBUG] Layer index {layer_index} out of range for {total_layers} layers, using last layer")
                            selected_layer_attention = step_attentions[-1]
                            actual_layer = total_layers - 1
                        
                        # åœ¨ç¬¬ä¸€ä¸ªstepæ—¶æ˜¾ç¤ºå±‚ä¿¡æ¯
                        if step_idx == 0 and verifier.verbose:
                            print(f"   [DEBUG] Total model layers: {total_layers}, using layer {actual_layer} (index: {layer_index})")
                        
                        last_layer_attention = selected_layer_attention
                        if isinstance(last_layer_attention, torch.Tensor):
                            # å¹³å‡æ‰€æœ‰å¤´
                            # last_layer_attention shape: [batch, heads, seq_len, seq_len]
                            # æ³¨æ„ï¼šæ¯ä¸ªstepçš„attentionçŸ©é˜µå¤§å°æ˜¯é€’å¢çš„
                            # step 0: shape [1, heads, context_length+1, context_length+1]
                            # step 1: shape [1, heads, context_length+2, context_length+2]
                            # ç­‰ç­‰...
                            
                            # è·å–å½“å‰åºåˆ—çš„æœ€åä¸€ä¸ªä½ç½®ï¼ˆå³åˆšç”Ÿæˆçš„tokenï¼‰
                            current_seq_len = last_layer_attention.shape[2]
                            last_pos = current_seq_len - 1
                            
                            # è·å–æœ€åä¸€ä¸ªtokenå¯¹æ‰€æœ‰ä¹‹å‰tokençš„æ³¨æ„åŠ›
                            avg_attention = last_layer_attention[0, :, last_pos, :].mean(dim=0)
                            
                            # å®é™…çš„åºåˆ—ä½ç½®
                            seq_pos = context_length + step_idx
                            verifier.update_attention_cache(seq_pos, avg_attention)
                            
                            if verifier.verbose and step_idx < 3:  # åªæ‰“å°å‰å‡ ä¸ª
                                print(f"   [DEBUG] Step {step_idx}: attention shape {last_layer_attention.shape}, extracted shape {avg_attention.shape}, position {seq_pos}")
            except Exception as e:
                if verifier.verbose:
                    print(f"   [DEBUG] Error processing generate attentions: {e}")
        else:
            print(f"   âš ï¸  WARNING: No attention weights in model output. Model may not support attention output.")
            print(f"   [DEBUG] Output attributes: {list(outputs.keys()) if hasattr(outputs, 'keys') else dir(outputs)[:5]}")
        
        # è§£ç è¾“å‡º
        generated_ids = outputs.sequences[0][context_length:]
        generated_text = tokenizer.decode(generated_ids, skip_special_tokens=True)
        
        print(f"\nç”Ÿæˆç»“æœ: {generated_text}")
        print(f"ç”Ÿæˆæ—¶é—´: {generation_time:.2f}ç§’")
        
        # æ”¶é›†ç»“æœ
        # æ­£ç¡®è§£ç tokens
        token_ids = outputs.sequences[0].tolist()
        all_decoded_tokens = []
        for token_id in token_ids:
            # è§£ç å•ä¸ªtokenï¼Œå»é™¤ç©ºæ ¼
            decoded_token = tokenizer.decode([token_id], skip_special_tokens=False)
            # å¤„ç†ç‰¹æ®Šæƒ…å†µ
            if decoded_token == '' or decoded_token == ' ':
                # å¯¹äºç©ºtokenï¼Œå°è¯•è·å–åŸå§‹tokenè¡¨ç¤º
                raw_token = tokenizer.convert_ids_to_tokens([token_id])[0]
                all_decoded_tokens.append(raw_token)
            else:
                all_decoded_tokens.append(decoded_token)
        
        # åˆ†ç¦»ä¸Šä¸‹æ–‡tokenså’Œç”Ÿæˆçš„tokens
        context_tokens = all_decoded_tokens[:context_length]
        generated_tokens = all_decoded_tokens[context_length:]
        
        result = {
            "test_case": {
                "name": test_case['name'],
                "category": test_case['category'],
                "description": test_case['description'],
                "system_prompt": test_case['system_prompt'][:200] + "...",  # æˆªæ–­ä»¥èŠ‚çœç©ºé—´
                "user_prompt": test_case['user_prompt']
            },
            "context_length": context_length,
            "system_prompt_length": system_prompt_length,  # ä¿å­˜ç³»ç»Ÿæç¤ºé•¿åº¦
            "generated_text": generated_text,
            "generation_time": generation_time,
            "verification_results": [asdict(r) for r in verifier.verification_results],
            "tokens": all_decoded_tokens,  # ä¿ç•™æ‰€æœ‰tokensç”¨äºå®Œæ•´æ˜¾ç¤º
            "attention_heatmap": {
                "tokens": all_decoded_tokens,  # åŒ…å«æ‰€æœ‰tokensä»¥ä¾¿æ­£ç¡®æ˜¾ç¤ºxè½´
                "attention_weights": verifier.get_full_attention_matrix(),  # åªåŒ…å«ç”Ÿæˆéƒ¨åˆ†çš„æ³¨æ„åŠ›
                "context_boundary": context_length,
                "system_prompt_boundary": system_prompt_length,  # æ–°å¢ï¼šç³»ç»Ÿæç¤ºè¾¹ç•Œ
                "generated_tokens": generated_tokens,  # æ˜ç¡®æ ‡è®°ç”Ÿæˆçš„tokens
                "context_tokens": context_tokens  # æ˜ç¡®æ ‡è®°ä¸Šä¸‹æ–‡tokens
            }
        }
        
        # æ‰“å°éªŒè¯ç»“æœæ‘˜è¦
        if verifier.verification_results:
            for vr in verifier.verification_results:
                print(f"\nğŸ“Š éªŒè¯ç»“æœ:")
                print(f"   åºåˆ—: {vr.sequence}")
                print(f"   è£å†³: {vr.final_verdict}")
                print(f"   äº‹å®æ€§å¾—åˆ†: {vr.factuality_score:.2f}")
                print(f"   ç³»ç»Ÿæ³¨æ„åŠ› vs ç”¨æˆ·æ³¨æ„åŠ›: {vr.avg_system_attention:.3f} vs {vr.avg_user_attention:.3f}")
                print(f"   é¢„æœŸè¡Œä¸º: {test_case['expected_behavior']}")
        
        all_results.append(result)
    
    # ç”Ÿæˆå‰ç«¯æ ¼å¼æ•°æ®
    frontend_results = generate_frontend_format(all_results)
    
    # ç›´æ¥ä¿å­˜åˆ°å‰ç«¯ public ç›®å½•
    frontend_output_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'frontend', 'public')
    os.makedirs(frontend_output_dir, exist_ok=True)
    
    output_file = os.path.join(frontend_output_dir, 'results.json')
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(frontend_results, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ… éªŒè¯å®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    
    # æ‰“å°æ€»ç»“
    print("\nğŸ“ˆ æµ‹è¯•æ€»ç»“:")
    print(f"   æµ‹è¯•ç”¨ä¾‹æ•°: {len(test_cases)}")
    
    # æŒ‰ç±»åˆ«ç»Ÿè®¡
    hallucination_cases = [tc for tc in test_cases if tc['category'] == 'hallucination']
    non_hallucination_cases = [tc for tc in test_cases if tc['category'] == 'non_hallucination']
    
    print(f"   - å¹»è§‰ç”Ÿæˆæµ‹è¯•: {len(hallucination_cases)} ä¸ª")
    print(f"   - æ­£å¸¸å¼•ç”¨æµ‹è¯•: {len(non_hallucination_cases)} ä¸ª")
    
    # ç»Ÿè®¡æ£€æµ‹ç»“æœ
    detected_sequences = sum(len(r['verification_results']) for r in all_results)
    detected_hallucinations = sum(
        1 for r in all_results 
        for vr in r['verification_results'] 
        if vr['is_hallucination']
    )
    
    print(f"\n   æ£€æµ‹åˆ°çš„æ•°å­—åºåˆ—: {detected_sequences} ä¸ª")
    print(f"   æ£€æµ‹ä¸ºå¹»è§‰çš„åºåˆ—: {detected_hallucinations} ä¸ª")
    

    
    return all_results


def generate_frontend_format(results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """ç”Ÿæˆå‰ç«¯å…¼å®¹çš„æ•°æ®æ ¼å¼"""
    print("\nğŸ“„ ç”Ÿæˆå‰ç«¯æ•°æ®æ ¼å¼...")
    
    frontend_results = []
    for result in results:
        # å¦‚æœæœ‰éªŒè¯ç»“æœï¼Œä¸ºæ¯ä¸ªç»“æœåˆ›å»ºä¸€ä¸ªæ¡ç›®
        if result.get('verification_results'):
            for vr in result['verification_results']:
                frontend_result = {
                    "test_case": result['test_case'],
                    "context_length": result['context_length'],
                    "generated_text": result['generated_text'],
                    "verification_result": vr,
                    "attention_heatmap": result.get('attention_heatmap', {
                        "tokens": [],
                        "attention_weights": [],
                        "context_boundary": result['context_length'],
                        "system_prompt_boundary": result.get('system_prompt_length', 0)
                    })
                }
                frontend_results.append(frontend_result)
        else:
            # å¦‚æœæ²¡æœ‰æ£€æµ‹åˆ°æ•°å­—åºåˆ—ï¼Œæ·»åŠ ä¸€ä¸ªé»˜è®¤ç»“æœ
            frontend_result = {
                "test_case": result['test_case'],
                "context_length": result['context_length'],
                "generated_text": result['generated_text'],
                "verification_result": {
                    "sequence": "N/A",
                    "tokens": [],
                    "factuality_score": 1.0,
                    "avg_system_attention": 0.0,
                    "avg_user_attention": 0.0,
                    "final_verdict": "NO_SEQUENCE_DETECTED",
                    "is_hallucination": False,
                    "analyses": [],
                    "verdict_details": {}
                },
                "attention_heatmap": result.get('attention_heatmap', {
                    "tokens": [],
                    "attention_weights": [],
                    "context_boundary": result['context_length'],
                    "system_prompt_boundary": result.get('system_prompt_length', 0)
                })
            }
            frontend_results.append(frontend_result)
    
    return frontend_results


if __name__ == "__main__":
    import sys
    
    # ä»å‘½ä»¤è¡Œå‚æ•°è·å–å±‚ç´¢å¼•ï¼ˆå¯é€‰ï¼‰
    attention_layer_index = -1  # é»˜è®¤ä½¿ç”¨æœ€åä¸€å±‚
    if len(sys.argv) > 1:
        try:
            attention_layer_index = int(sys.argv[1])
            print(f"ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°æŒ‡å®šçš„æ³¨æ„åŠ›å±‚ç´¢å¼•: {attention_layer_index}")
        except ValueError:
            print(f"è­¦å‘Š: æ— æ•ˆçš„å±‚ç´¢å¼•å‚æ•° '{sys.argv[1]}'ï¼Œä½¿ç”¨é»˜è®¤å€¼ -1ï¼ˆæœ€åä¸€å±‚ï¼‰")
    
    # è¿è¡ŒéªŒè¯
    results = run_verification(attention_layer_index=attention_layer_index)